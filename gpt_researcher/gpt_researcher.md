
# GPT Researcher Documentation

**Last Updated:** September 22, 2023

**Author:** Assaf Elovic

**Mission:** To empower individuals and organizations with accurate, unbiased, and factual information through AI.

---

## Table of Contents
1.  [Introduction](#introduction)
2.  [Why GPT Researcher?](#why-gpt-researcher)
3.  [Architecture](#architecture)
4.  [How GPT Researcher was Built](#how-gpt-researcher-was-built)
    *   [Moving from Infinite Loops to Deterministic Results](#moving-from-infinite-loops-to-deterministic-results)
    *   [Aiming for Objective and Unbiased Results](#aiming-for-objective-and-unbiased-results)
    *   [Speeding up the Research Process](#speeding-up-the-research-process)
    *   [Finalizing the Research Report](#finalizing-the-research-report)
    *   [The Final Architecture](#the-final-architecture)
    *   [Going Forward](#going-forward)
5.  [Features](#features)
6.  [Getting Started](#getting-started)
    *   [Installation](#installation)
    *   [Run as PIP Package](#run-as-pip-package)
    *   [Run with Docker](#run-with-docker)
    *   [Research on Local Documents](#research-on-local-documents)
7.  [Multi-Agent Assistant](#multi-agent-assistant)
8.  [Frontend Applications](#frontend-applications)
    *    [How to Choose](#how-to-choose)
    *    [Options](#options)
    *    [Usage Options](#usage-options)
        1. [PIP Package](#1-pip-package)
        2. [End-to-End Application](#2-end-to-end-application)
        3. [Multi Agent System with LangGraph](#3-multi-agent-system-with-langgraph)
    *    [Comparison Table](#comparison-table)
    *    [Deep Dive](#deep-dive)
    *    [Versioning and Updates](#versioning-and-updates)
9.  [PIP Package Details](#pip-package-details)
    *   [Installation](#installation-1)
    *   [Example Usage](#example-usage)
    *   [Specific Examples](#specific-examples)
    *   [Integration with Web Frameworks (FastAPI & Flask)](#integration-with-web-frameworks-fastapi--flask)
    *   [Getters and Setters](#getters-and-setters)
    *   [Advanced Usage](#advanced-usage)
10. [Configuration](#configuration)
11. [Scraping Options](#scraping-options)
    *    [Configuring Scraping Method](#configuring-scraping-method)
    *    [Scraping Methods Explained](#scraping-methods-explained)
    *    [Additional Setup for Selenium](#additional-setup-for-selenium)
    *    [Choosing the Right Method](#choosing-the-right-method)
    *    [Troubleshooting](#troubleshooting)

---

## Introduction

GPT Researcher is an autonomous agent designed for comprehensive web and local research on any given task. It produces detailed, factual, and unbiased research reports with citations.  GPT Researcher offers a full suite of customization options to create tailor-made and domain-specific research agents. Inspired by the "Plan-and-Solve" and RAG papers, it addresses misinformation, speed, determinism, and reliability by offering stable performance and increased speed through parallelized agent work.

## Why GPT Researcher?

Traditional manual research is time-consuming and resource-intensive.  Existing LLMs face several challenges:

*   **Outdated Information:** LLMs trained on outdated data can hallucinate, making them unreliable for current research.
*   **Token Limitations:**  Current LLMs have token limits that are insufficient for generating long, detailed research reports.
*   **Limited Web Sources:**  Many existing services use limited web sources, leading to shallow results and potential misinformation.
*   **Bias:** Selective web sources can introduce bias into research tasks.
* **Time Consuming:** Objective conclusions for manual research can take weeks.

## Architecture

The core architecture of GPT Researcher utilizes "planner" and "execution" agents.

1.  **Planner Agent:** Generates research questions related to the main research query.
2.  **Execution Agents:**  Gather relevant information for each research question generated by the planner.
3.  **Publisher:** Aggregates all findings from the execution agents into a comprehensive final report.

**Steps:**

1.  Create a task-specific agent based on a research query.
2.  Generate questions that collectively form an objective opinion on the task.
3.  Use a crawler agent for gathering information for each question.
4.  Summarize and source-track each resource.
5.  Filter and aggregate summaries into a final research report.

## How GPT Researcher was Built

### Moving from Infinite Loops to Deterministic Results

Inspired by the "Plan and Solve" paper, GPT Researcher adopts a deterministic approach:

*   **Planning:**  First, an outline of research questions is created, dividing the main task into smaller subtasks.
*   **Execution:**  Agents are executed deterministically for each outline item (research question).

This approach eliminates infinite loops and ensures task completion within a finite timeframe.

### Aiming for Objective and Unbiased Results

To address factuality and bias:

*   **Law of Large Numbers:**  Gathering information from a large number of sources reduces bias.
*   **LLM Summarization:**  Leveraging LLMs to summarize factual information from multiple sources improves factuality.

The system scrapes a sufficient number of relevant websites to form an objective opinion.

### Speeding up the Research Process

GPT Researcher utilizes parallel processing using Python's `asyncio` library to significantly speed up the research process.  Agent tasks are executed concurrently.

```python
# Example of parallel scraping
tasks = [async_browse(url, query, self.websocket) for url in await new_search_urls]
responses = await asyncio.gather(*tasks, return_exceptions=True)
```

This parallelization reduces the average research time to approximately three minutes, an 85% improvement compared to synchronous approaches like AutoGPT.

### Finalizing the Research Report

The final report is generated by providing all aggregated information to a powerful LLM (GPT-4 by default) with a specific prompt:

> "{research_summary}" Using the above information, answer the following question or topic: "{question}" in a detailed report â€” The report should focus on the answer to the question, should be well structured, informative, in depth, with facts and numbers if available, a minimum of 1,200 words and with markdown syntax and apa format. Write all source urls at the end of the report in apa format. You should write your report only based on the given information and nothing else.

### The Final Architecture

1.  **Generate Outline:** Create research questions to form an objective opinion on the task.
2.  **Crawler Agents:** For each question, trigger a crawler agent to scrape relevant online resources.
3.  **Filter and Summarize:** Track, filter, and summarize each scraped resource, keeping only relevant information.
4.  **Aggregate and Generate Report:** Aggregate all summarized sources and generate the final research report.

### Going Forward

The future of online research automation involves AI agents capable of understanding and analyzing various forms of online content (videos, images, graphs, etc.) and handling vast amounts of information.  GPT Researcher aims to be a versatile research agent, easily integrable into existing agent workflows.

## Features

*   ðŸ“ **Detailed Research Reports:** Generates comprehensive research reports using web and local documents.
*   ðŸ–¼ï¸ **Smart Image Scraping:** Includes smart image scraping and filtering.
*   ðŸ“œ **Long Reports:**  Generates reports exceeding 2,000 words.
*   ðŸŒ **Source Aggregation:** Aggregates over 20 sources for objective conclusions.
*   ðŸ–¥ï¸ **Frontend Options:** Offers lightweight (HTML/CSS/JS) and production-ready (NextJS + Tailwind) frontend versions.
*   ðŸ” **JavaScript-Enabled Scraping:** Supports JavaScript-enabled web scraping.
*   ðŸ“‚ **Contextual Memory:** Maintains memory and context throughout the research process.
*   ðŸ“„ **Multiple Export Formats:** Exports reports to PDF, Word, and other formats.

## Getting Started

### Installation

1.  **Python:** Install Python 3.11 or later.
2.  **Clone Repository:**
    ```bash
    git clone https://github.com/assafelovic/gpt-researcher.git
    cd gpt-researcher
    ```
3.  **API Keys:** Set up API keys (OpenAI and Tavily) by exporting them or storing them in a `.env` file:
    ```bash
    export OPENAI_API_KEY={Your OpenAI API Key here}
    export TAVILY_API_KEY={Your Tavily API Key here}
    ```
4.  **Install Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
5.  **Start Server:**
    ```bash
    python -m uvicorn main:app --reload
    ```
6.  **Access:** Visit `http://localhost:8000` to start.

For other setups (e.g., Poetry or virtual environments), refer to the [Getting Started](https://docs.tavily.com/researcher/) page.

### Run as PIP Package

```bash
pip install gpt-researcher
```

**Example Usage:**

```python
from gpt_researcher import GPTResearcher

query = "why is Nvidia stock going up?"
researcher = GPTResearcher(query=query, report_type="research_report")
# Conduct research on the given query
research_result = await researcher.conduct_research()
# Write the report
report = await researcher.write_report()
```
For more examples and configurations, refer to the [PIP documentation](https://python.langchain.com/docs/integrations/tools/gpt_researcher) page.

### Run with Docker

1.  **Install Docker:** Install Docker on your system.
2.  **Environment Variables:** Clone the `.env.example` file, add your API keys, and save it as `.env`.
3.  **Docker Compose:**  In the `docker-compose.yml` file, comment out services you don't need.
4.  **Build and Run:**
    ```bash
    docker-compose up --build
    ```
    or
    ```bash
    docker compose up --build
    ```
5.  **Access:** Visit `localhost:3000` in your browser.

### Research on Local Documents

1.  **Set DOC\_PATH:** Add the `DOC_PATH` environment variable pointing to the folder containing your documents:

    ```bash
    export DOC_PATH="./my-docs"
    ```

2.  **Select Source:**
    *   **Frontend App:** Select "My Documents" from the "Report Source" dropdown.
    *   **PIP Package:** Pass `report_source="local"` when instantiating the `GPTResearcher` class.

Supported file formats: PDF, plain text, CSV, Excel, Markdown, PowerPoint, and Word documents.

## Multi-Agent Assistant

GPT Researcher features a multi-agent assistant built with LangGraph, inspired by the STORM paper. This enhances research depth and quality by leveraging multiple specialized agents.

*   Generates 5-6 page research reports in multiple formats (PDF, Docx, Markdown).

Check it out [here](https://github.com/assafelovic/gpt-researcher/tree/main/examples/langgraph_multi_agent) or refer to the [documentation](https://python.langchain.com/docs/use_cases/research/multi_agent_research) for more information.

## Frontend Applications

GPT-Researcher provides an enhanced frontend with:

*   Intuitive interface for research queries.
*   Real-time progress tracking.
*   Interactive display of findings.
*   Customizable settings.

**Deployment Options:**

*   **Lightweight Static Frontend:** Served by FastAPI.
*   **Feature-Rich NextJS Application:** For advanced functionality.

For setup instructions and more information, visit the [documentation page](https://docs.tavily.com/researcher/).

### How to Choose
GPT Researcher offers flexible options to meet your needs.

### Options
GPT Researcher offers multiple ways to leverage its capabilities:

### Usage Options

#### 1. PIP Package

Ideal for integrating GPT Researcher into existing projects and workflows.

**Pros:**

*   Easy integration
*   Flexible usage
*   Optimized for production

**Cons:**

*   Requires coding knowledge
*   May need additional setup

**Installation:**

```bash
pip install gpt-researcher
```

**System Requirements:**

*   Python 3.10+
*   pip

[Learn More: PIP Documentation](https://python.langchain.com/docs/integrations/tools/gpt_researcher)

#### 2. End-to-End Application

Provides a complete out-of-the-box experience with a frontend.

**Pros:**

*   Ready-to-use frontend and backend
*   Includes advanced features
*   Optimal user experience

**Cons:**

*   Less flexible for custom integrations
*   Requires setting up the entire application

**Getting Started:**

*   Clone the repository:  `git clone https://github.com/assafelovic/gpt-researcher.git`
*   Follow the installation instructions

**System Requirements:**

*   Git
*   Python 3.10+
*   Node.js and npm (for frontend)

[Advanced Usage Example: Detailed Report Implementation](https://python.langchain.com/docs/integrations/tools/gpt_researcher#detailed-report-implementation)

#### 3. Multi Agent System with LangGraph

Offers the most comprehensive research capabilities using LangGraph.

**Pros:**

*   Detailed, customized reports
*   Inner AI agent loops and reasoning

**Cons:**

*   More expensive and time-consuming
*   Heavyweight for production

Recommended for local, experimental, and educational use.

**System Requirements:**

*   Python 3.10+
*   LangGraph library

[Learn More: GPT Researcher x LangGraph](https://python.langchain.com/docs/use_cases/research/multi_agent_research)

### Comparison Table

| Feature             | PIP Package | End-to-End Application | Multi Agent System |
| ------------------- | ----------- | ---------------------- | ------------------ |
| Ease of Integration | High        | Medium                 | Low                |
| Customization       | High        | Medium                 | High               |
| Out-of-the-box UI   | No          | Yes                    | No                 |
| Complexity          | Low         | Medium                 | High               |
| Best for            | Developers  | End-users              | Researchers        |

### Deep Dive

*   **PIP Package:**
    *   Install:  `pip install gpt-researcher`
    *   [Integration guide](https://python.langchain.com/docs/integrations/tools/gpt_researcher)
*   **End-to-End Application:**
    *   Clone: `git clone https://github.com/assafelovic/gpt-researcher.git`
    *   [Installation instructions](https://docs.tavily.com/researcher/)
*   **Multi-Agent System:**
    *   [Multi-Agents code](https://github.com/assafelovic/gpt-researcher/tree/main/examples/langgraph_multi_agent)
    *   [LangGraph documentation](https://python.langchain.com/docs/langgraph)
    *   [Blog](https://blog.langchain.dev/multi-agent- à®¨à®²à¯à®²à®¤à¯-research-with-gpt-researcher-and-langgraph/)

### Versioning and Updates

*   **PIP Package:** `pip install --upgrade gpt-researcher`
*   **End-to-End Application:** Pull the latest changes from the GitHub repository.
*   **Multi-Agent System:** Check documentation for compatibility.

## PIP Package Details

### Installation

1.  **Prerequisite:** Python 3.10+
2.  **Install:** `pip install gpt-researcher`
3.  **Environment Variables:** Create a `.env` file or export:

    ```bash
    export OPENAI_API_KEY={Your OpenAI API Key here}
    export TAVILY_API_KEY={Your Tavily API Key here}
    ```

### Example Usage

```python
from gpt_researcher import GPTResearcher
import asyncio

async def get_report(query: str, report_type: str):
    researcher = GPTResearcher(query, report_type)
    research_result = await researcher.conduct_research()
    report = await researcher.write_report()

    # Get additional information
    research_context = researcher.get_research_context()
    research_costs = researcher.get_costs()
    research_images = researcher.get_research_images()
    research_sources = researcher.get_research_sources()

    return report, research_context, research_costs, research_images, research_sources

if __name__ == "__main__":
    query = "what team may win the NBA finals?"
    report_type = "research_report"

    report, context, costs, images, sources = asyncio.run(get_report(query, report_type))

    print("Report:")
    print(report)
    print("\nResearch Costs:")
    print(costs)
    print("\nNumber of Research Images:")
    print(len(images))
    print("\nNumber of Research Sources:")
    print(len(sources))

```

### Specific Examples

*   **Research Report:**
    ```python
    query = "Latest developments in renewable energy technologies"
    report_type = "research_report"
    ```

*   **Resource Report:**
    ```python
    query = "List of top AI conferences in 2023"
    report_type = "resource_report"
    ```

*   **Outline Report:**
    ```python
    query = "Outline for an article on the impact of AI in education"
    report_type = "outline_report"
    ```

### Integration with Web Frameworks (FastAPI & Flask)

**FastAPI Example:**

```python
from fastapi import FastAPI
from gpt_researcher import GPTResearcher
import asyncio

app = FastAPI()

@app.get("/report/{report_type}")
async def get_report(query: str, report_type: str) -> dict:
    researcher = GPTResearcher(query, report_type)
    research_result = await researcher.conduct_research()
    report = await researcher.write_report()

    source_urls = researcher.get_source_urls()
    research_costs = researcher.get_costs()
    research_images = researcher.get_research_images()
    research_sources = researcher.get_research_sources()

    return {
        "report": report,
        "source_urls": source_urls,
        "research_costs": research_costs,
        "num_images": len(research_images),
        "num_sources": len(research_sources)
    }

# Run the server
# uvicorn main:app --reload
```

**Flask Example:**

```python
# Pre-requisite: Install flask with the async extra.
# pip install 'flask[async]'

from flask import Flask, request, jsonify
from gpt_researcher import GPTResearcher

app = Flask(__name__)

@app.route('/report/<report_type>', methods=['GET'])
async def get_report(report_type):
    query = request.args.get('query')
    researcher = GPTResearcher(query, report_type)
    research_result = await researcher.conduct_research()
    report = await researcher.write_report()

    source_urls = researcher.get_source_urls()
    research_costs = researcher.get_costs()
    research_images = researcher.get_research_images()
    research_sources = researcher.get_research_sources()

    return jsonify({
        "report": report,
        "source_urls": source_urls,
        "research_costs": research_costs,
        "num_images": len(research_images),
        "num_sources": len(research_sources)
    })

# Run the server
# flask run
```
**Example Request (Flask):**
```bash
curl -X GET "http://localhost:5000/report/research_report?query=what team may win the nba finals?"
```

### Getters and Setters

*   **Get Research Sources:** `source_urls = researcher.get_source_urls()`
*   **Get Research Context:** `research_context = researcher.get_research_context()`
*   **Get Research Costs:** `research_costs = researcher.get_costs()`
*   **Get Research Images:** `research_images = researcher.get_research_images()`
*   **Get Research Sources:** `research_sources = researcher.get_research_sources()`
*   **Set Verbose:** `researcher.set_verbose(True)`
*   **Add Costs:** `researcher.add_costs(0.22)`

### Advanced Usage

**Customizing the Research Process:**

```python
researcher = GPTResearcher(
    query="Your research query",
    report_type="research_report",
    report_format="APA",
    tone="formal and objective",
    max_subtopics=5,
    verbose=True
)
```

**Handling Research Results:**

```python
# Conduct research
research_result = await researcher.conduct_research()

# Generate a report
report = await researcher.write_report()

# Generate a conclusion
conclusion = await researcher.write_report_conclusion(report)

# Get subtopics
subtopics = await researcher.get_subtopics()

# Get draft section titles for a subtopic
draft_titles = await researcher.get_draft_section_titles("Subtopic name")

# Get similar written contents based on draft section titles
similar_contents = await researcher.get_similar_written_contents_by_draft_section_titles(
    current_subtopic="Subtopic name",
    draft_section_titles=["Title 1", "Title 2"],
    written_contents=some_written_contents,
    max_results=10
)
# Get the full research context
context = researcher.get_research_context()
```

## Configuration

The `config.py` file (located in `/gpt_researcher/config/`) allows customization of GPT Researcher.  It supports multiple LLMs, Retrievers, and report formats. You can also use a custom JSON config file.

**Supported Options:**

*   `RETRIEVER`: Web search engine (default: `tavily`). Options: `duckduckgo`, `bing`, `google`, `searchapi`, `serper`, `searx`.
*   `EMBEDDING`: Embedding model (default: `openai:text-embedding-3-small`). Options: `ollama`, `huggingface`, `azure_openai`, `custom`.
*   `FAST_LLM`: Model for fast operations (default: `openai:gpt-4o-mini`).
*   `SMART_LLM`: Model for smart operations (default: `openai:gpt-4o`).
*   `STRATEGIC_LLM`: Model for strategic operations (default: `openai:o1-preview`).
*   `LANGUAGE`: Language for the report (default: `english`).
*   `CURATE_SOURCES`: Whether to curate sources for research (improves quality, increases cost) (default: `True`).
*   `FAST_TOKEN_LIMIT`: Token limit for fast LLM (default: `2000`).
*   `SMART_TOKEN_LIMIT`: Token limit for smart LLM (default: `4000`).
*   `STRATEGIC_TOKEN_LIMIT`: Token limit for strategic LLM (default: `4000`).
*   `BROWSE_CHUNK_MAX_LENGTH`: Max chunk length for browsing (default: `8192`).
*   `SUMMARY_TOKEN_LIMIT`: Token limit for summaries (default: `700`).
*   `TEMPERATURE`: Sampling temperature for LLM (default: `0.55`).
*   `TOTAL_WORDS`: Word count limit (default: `800`).
*   `REPORT_FORMAT`: Report format (default: `APA`).  Other options: `MLA`, `CMS`, `Harvard style`, `IEEE`, etc.
*   `MAX_ITERATIONS`: Max iterations for query expansion (default: `3`).
*   `AGENT_ROLE`: Role of the agent (no default).
*   `MAX_SUBTOPICS`: Max subtopics (default: `3`).
*   `SCRAPER`: Web scraper (default: `bs` - BeautifulSoup).  Other options: `browser` (Selenium), `tavily_extract`.
*   `DOC_PATH`: Path for local documents (default: `` - empty string).
*   `USER_AGENT`: Custom User-Agent string.
*   `MEMORY_BACKEND`: Memory backend (default: `local`).

**Changing Defaults:**

Set environment variables in your `.env` file or export them manually:

```bash
export RETRIEVER=bing
export REPORT_FORMAT=IEEE
```

You may need additional API keys for other retrievers and LLMs.  Follow console logs for guidance.  For more on LLM support, see the [docs](https://python.langchain.com/docs/integrations/llms/).

## Scraping Options

GPT Researcher offers different web scraping methods:

*   **BeautifulSoup (Static):**  `SCRAPER="bs"` (Default)
    *   Fast and lightweight.
    *   Sends a single HTTP request.
    *   Parses static HTML.
    *   Cannot handle dynamic content loaded by JavaScript.

*   **Selenium (Dynamic):** `SCRAPER="browser"`
    *   Opens a browser instance (Chrome by default).
    *   Executes JavaScript.
    *   Handles dynamic content.
    *   Slower and requires more resources.
    *   Requires Selenium and WebDriver installation.

*   **Tavily Extract (Recommended for Production):** `SCRAPER="tavily_extract"`
    *   Uses Tavily's infrastructure for scalable scraping.
    *   Handles CAPTCHAs, JavaScript rendering, and anti-bot measures.
    *   Provides clean, structured content.
    *   Requires a Tavily account and API key.
    *   Install `tavily-python`: `pip install tavily-python`
    *   Set API key: `export TAVILY_API_KEY="your-api-key"`

### Configuring Scraping Method

Set the `SCRAPER` environment variable:

*   **BeautifulSoup:** `export SCRAPER="bs"`
*   **Selenium:** `export SCRAPER="browser"`
*   **Tavily Extract:** `export SCRAPER="tavily_extract"`

### Scraping Methods Explained
**BeautifulSoup (Static Scraping)**
When SCRAPER="bs", GPT Researcher uses BeautifulSoup for static scraping. This method:

*   Sends a single HTTP request to fetch the page content
*   Parses the static HTML content
*   Extracts text and data from the parsed HTML

**Benefits:**

*   Faster and more lightweight
*   Doesn't require additional setup
*   Works well for simple, static websites

**Limitations:**

*   Cannot handle dynamic content loaded by JavaScript
*   May miss content that requires user interaction to display

**Selenium (Browser Scraping)**
When SCRAPER="browser", GPT Researcher uses Selenium for dynamic scraping. This method:

*   Opens a real browser instance (Chrome by default)
*   Loads the page and executes JavaScript
*   Waits for dynamic content to load
*   Extracts text and data from the fully rendered page

**Benefits:**

*   Can scrape dynamically loaded content
*   Simulates real user interactions (scrolling, clicking, etc.)
*   Works well for complex, JavaScript-heavy websites

**Limitations:**

*   Slower than static scraping
*   Requires more system resources
*   Requires additional setup (Selenium and WebDriver installation)

**Tavily Extract (Recommended for Production)**
When SCRAPER="tavily_extract", GPT Researcher uses Tavily's Extract API for web scraping. This method:

*   Uses Tavily's robust infrastructure to handle web scraping at scale
*   Automatically handles CAPTCHAs, JavaScript rendering, and anti-bot measures
*   Provides clean, structured content extraction

**Benefits:**

*   Production-ready and highly reliable
*   No need to manage proxies or handle rate limiting
*   Excellent success rate on most websites
*   Handles both static and dynamic content
*   Built-in content cleaning and formatting
*   Fast response times through Tavily's distributed infrastructure

**Setup:**

*   Create a Tavily account at app.tavily.com
*   Get your API key from the dashboard
*   Install the Tavily Python SDK:
```bash
pip install tavily-python
```
*Set your Tavily API key:
```bash
export TAVILY_API_KEY="your-api-key"
```
**Usage Considerations:**

*   Requires a Tavily API key and account
*   API calls are metered based on your Tavily plan
*   Best for production environments where reliability is crucial
*   Ideal for businesses and applications that need consistent scraping results

### Additional Setup for Selenium

If using Selenium (`SCRAPER="browser"`):

1.  **Install Selenium:** `pip install selenium`
2.  **Download WebDriver:**
    *   Chrome: [ChromeDriver](https://sites.google.com/chromium.org/driver/)
    *   Firefox: [GeckoDriver](https://github.com/mozilla/geckodriver/releases)
    *   Safari: Built-in, no download required.
3.  Ensure WebDriver is in your system's `PATH`.

### Choosing the Right Method

*   **BeautifulSoup (Static):** Simple, static websites; speed is a priority.
*   **Selenium (Dynamic):**  JavaScript-heavy websites; requires simulating user interactions.
*   **Tavily Extract:** Production use, reliable and consistent results.

### Troubleshooting

*   **Selenium Fails:** Ensure correct WebDriver is installed and in your `PATH`.
*   `ImportError` (Selenium):  Make sure you've installed the `selenium` package.
*   **Missing Content:** Try switching between static and dynamic scraping.

Choose the scraping method that best suits your research needs and target websites.
